# ============================================
# Nexus LMS - GitLab CI/CD Pipeline
# Complete CI/CD with Docker, K8s, and AWS
# ============================================

stages:
  - validate
  - test
  - build
  - security
  - deploy-dev
  - deploy-staging
  - deploy-prod

# ─────────────────────────────────────────
# Global Variables
# ─────────────────────────────────────────
variables:
  # Docker configuration
  DOCKER_HOST: tcp://docker:2376
  DOCKER_TLS_CERTDIR: "/certs"
  DOCKER_TLS_VERIFY: 1
  DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
  
  # AWS Configuration (set in GitLab CI/CD settings)
  AWS_DEFAULT_REGION: ap-south-1
  
  # Application
  CLIENT_IMAGE: nexus-client
  SERVER_IMAGE: nexus-server
  
  # Kubernetes
  KUBE_NAMESPACE_DEV: nexus-lms-dev
  KUBE_NAMESPACE_STAGING: nexus-lms-staging
  KUBE_NAMESPACE_PROD: nexus-lms-prod

# ─────────────────────────────────────────
# Default Configuration
# ─────────────────────────────────────────
default:
  image: docker:24
  services:
    - docker:24-dind
  before_script:
    - apk add --no-cache curl bash aws-cli kubectl git

# ─────────────────────────────────────────
# Cache Configuration
# ─────────────────────────────────────────
.node_cache: &node_cache
  cache:
    key: ${CI_COMMIT_REF_SLUG}
    paths:
      - node_modules/
      - client/node_modules/
      - server/node_modules/
    policy: pull-push

# ─────────────────────────────────────────
# STAGE: Validate
# ─────────────────────────────────────────
lint:
  stage: validate
  image: node:20-alpine
  <<: *node_cache
  script:
    - npm ci
    - npm run lint --workspaces --if-present
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_COMMIT_BRANCH == "develop"'

typecheck:
  stage: validate
  image: node:20-alpine
  <<: *node_cache
  script:
    - npm ci
    - npm run typecheck --workspaces --if-present
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_COMMIT_BRANCH == "develop"'

terraform-validate:
  stage: validate
  image: hashicorp/terraform:1.6
  script:
    - cd terraform
    - terraform init -backend=false
    - terraform validate
    - terraform fmt -check -recursive
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      changes:
        - terraform/**/*
    - if: '$CI_COMMIT_BRANCH == "main"'
      changes:
        - terraform/**/*

# ─────────────────────────────────────────
# STAGE: Test
# ─────────────────────────────────────────
test-server:
  stage: test
  image: node:20-alpine
  <<: *node_cache
  services:
    - name: postgres:16-alpine
      alias: postgres
  variables:
    POSTGRES_USER: test
    POSTGRES_PASSWORD: test
    POSTGRES_DB: nexus_test
    DATABASE_URL: postgresql://test:test@postgres:5432/nexus_test
  script:
    - npm ci
    - cd server
    - npx prisma generate
    - npx prisma db push
    - npm test
  coverage: '/All files[^|]*\|[^|]*\s+([\d\.]+)/'
  artifacts:
    reports:
      junit: server/coverage/junit.xml
      coverage_report:
        coverage_format: cobertura
        path: server/coverage/cobertura-coverage.xml
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_COMMIT_BRANCH == "develop"'

test-client:
  stage: test
  image: node:20-alpine
  <<: *node_cache
  script:
    - npm ci
    - cd client
    - npm test -- --run --coverage
  coverage: '/All files[^|]*\|[^|]*\s+([\d\.]+)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: client/coverage/cobertura-coverage.xml
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_COMMIT_BRANCH == "develop"'

# ─────────────────────────────────────────
# STAGE: Build
# ─────────────────────────────────────────
.build_template: &build_template
  stage: build
  script:
    # Login to ECR
    - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $ECR_REGISTRY
    
    # Build and push with cache
    - |
      docker build \
        --cache-from $ECR_REGISTRY/$IMAGE_NAME:latest \
        --build-arg BUILDKIT_INLINE_CACHE=1 \
        -t $ECR_REGISTRY/$IMAGE_NAME:$CI_COMMIT_SHA \
        -t $ECR_REGISTRY/$IMAGE_NAME:$CI_COMMIT_REF_SLUG \
        -t $ECR_REGISTRY/$IMAGE_NAME:latest \
        -f docker/$DOCKERFILE .
    
    - docker push $ECR_REGISTRY/$IMAGE_NAME --all-tags
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_COMMIT_BRANCH == "develop"'

build-client:
  <<: *build_template
  variables:
    IMAGE_NAME: $CLIENT_IMAGE
    DOCKERFILE: Dockerfile.client

build-server:
  <<: *build_template
  variables:
    IMAGE_NAME: $SERVER_IMAGE
    DOCKERFILE: Dockerfile.server

# ─────────────────────────────────────────
# STAGE: Security
# ─────────────────────────────────────────
container-scan-client:
  stage: security
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity CRITICAL,HIGH $ECR_REGISTRY/$CLIENT_IMAGE:$CI_COMMIT_SHA
  allow_failure: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_COMMIT_BRANCH == "develop"'
  needs:
    - build-client

container-scan-server:
  stage: security
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity CRITICAL,HIGH $ECR_REGISTRY/$SERVER_IMAGE:$CI_COMMIT_SHA
  allow_failure: true
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_COMMIT_BRANCH == "develop"'
  needs:
    - build-server

sast:
  stage: security
  image: node:20-alpine
  script:
    - npm ci
    - npx snyk test --all-projects
  allow_failure: true
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_COMMIT_BRANCH == "main"'

# ─────────────────────────────────────────
# STAGE: Deploy Development
# ─────────────────────────────────────────
deploy-dev:
  stage: deploy-dev
  image: alpine/k8s:1.29.0
  environment:
    name: development
    url: https://dev.nexus-lms.com
  script:
    # Configure kubectl
    - aws eks update-kubeconfig --region $AWS_DEFAULT_REGION --name $EKS_CLUSTER_DEV
    
    # Create namespace if not exists
    - kubectl create namespace $KUBE_NAMESPACE_DEV --dry-run=client -o yaml | kubectl apply -f -
    
    # Apply secrets from AWS Secrets Manager
    - |
      kubectl create secret generic nexus-secrets \
        --namespace $KUBE_NAMESPACE_DEV \
        --from-literal=DATABASE_URL="$(aws secretsmanager get-secret-value --secret-id nexus-lms-dev/app-secrets --query SecretString --output text | jq -r .DATABASE_URL)" \
        --from-literal=JWT_SECRET="$(aws secretsmanager get-secret-value --secret-id nexus-lms-dev/app-secrets --query SecretString --output text | jq -r .JWT_SECRET)" \
        --from-literal=JWT_REFRESH_SECRET="$(aws secretsmanager get-secret-value --secret-id nexus-lms-dev/app-secrets --query SecretString --output text | jq -r .JWT_REFRESH_SECRET)" \
        --dry-run=client -o yaml | kubectl apply -f -
    
    # Deploy using Kustomize
    - |
      cd k8s/overlays/dev
      kustomize edit set image \
        nexus-client=$ECR_REGISTRY/$CLIENT_IMAGE:$CI_COMMIT_SHA \
        nexus-server=$ECR_REGISTRY/$SERVER_IMAGE:$CI_COMMIT_SHA
      kustomize build . | kubectl apply -f -
    
    # Wait for rollout
    - kubectl rollout status deployment/nexus-client-dev -n $KUBE_NAMESPACE_DEV --timeout=300s
    - kubectl rollout status deployment/nexus-server-dev -n $KUBE_NAMESPACE_DEV --timeout=300s
    
    # Run database migrations
    - |
      kubectl run prisma-migrate-$CI_PIPELINE_ID \
        --namespace $KUBE_NAMESPACE_DEV \
        --image=$ECR_REGISTRY/$SERVER_IMAGE:$CI_COMMIT_SHA \
        --restart=Never \
        --rm \
        --wait \
        --command -- npx prisma migrate deploy
  rules:
    - if: '$CI_COMMIT_BRANCH == "develop"'
  needs:
    - build-client
    - build-server
    - container-scan-client
    - container-scan-server

# ─────────────────────────────────────────
# STAGE: Deploy Staging
# ─────────────────────────────────────────
deploy-staging:
  stage: deploy-staging
  image: alpine/k8s:1.29.0
  environment:
    name: staging
    url: https://staging.nexus-lms.com
  script:
    - aws eks update-kubeconfig --region $AWS_DEFAULT_REGION --name $EKS_CLUSTER_STAGING
    - kubectl create namespace $KUBE_NAMESPACE_STAGING --dry-run=client -o yaml | kubectl apply -f -
    
    # Apply secrets
    - |
      kubectl create secret generic nexus-secrets \
        --namespace $KUBE_NAMESPACE_STAGING \
        --from-literal=DATABASE_URL="$(aws secretsmanager get-secret-value --secret-id nexus-lms-staging/app-secrets --query SecretString --output text | jq -r .DATABASE_URL)" \
        --from-literal=JWT_SECRET="$(aws secretsmanager get-secret-value --secret-id nexus-lms-staging/app-secrets --query SecretString --output text | jq -r .JWT_SECRET)" \
        --from-literal=JWT_REFRESH_SECRET="$(aws secretsmanager get-secret-value --secret-id nexus-lms-staging/app-secrets --query SecretString --output text | jq -r .JWT_REFRESH_SECRET)" \
        --dry-run=client -o yaml | kubectl apply -f -
    
    # Deploy
    - |
      cd k8s/overlays/staging
      kustomize edit set image \
        nexus-client=$ECR_REGISTRY/$CLIENT_IMAGE:$CI_COMMIT_SHA \
        nexus-server=$ECR_REGISTRY/$SERVER_IMAGE:$CI_COMMIT_SHA
      kustomize build . | kubectl apply -f -
    
    - kubectl rollout status deployment/nexus-client-staging -n $KUBE_NAMESPACE_STAGING --timeout=300s
    - kubectl rollout status deployment/nexus-server-staging -n $KUBE_NAMESPACE_STAGING --timeout=300s
    
    # Database migrations
    - |
      kubectl run prisma-migrate-$CI_PIPELINE_ID \
        --namespace $KUBE_NAMESPACE_STAGING \
        --image=$ECR_REGISTRY/$SERVER_IMAGE:$CI_COMMIT_SHA \
        --restart=Never \
        --rm \
        --wait \
        --command -- npx prisma migrate deploy
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  needs:
    - build-client
    - build-server
    - container-scan-client
    - container-scan-server

# ─────────────────────────────────────────
# STAGE: Deploy Production
# ─────────────────────────────────────────
deploy-prod:
  stage: deploy-prod
  image: alpine/k8s:1.29.0
  environment:
    name: production
    url: https://nexus-lms.com
  script:
    - aws eks update-kubeconfig --region $AWS_DEFAULT_REGION --name $EKS_CLUSTER_PROD
    - kubectl create namespace $KUBE_NAMESPACE_PROD --dry-run=client -o yaml | kubectl apply -f -
    
    # Apply secrets
    - |
      kubectl create secret generic nexus-secrets \
        --namespace $KUBE_NAMESPACE_PROD \
        --from-literal=DATABASE_URL="$(aws secretsmanager get-secret-value --secret-id nexus-lms-prod/app-secrets --query SecretString --output text | jq -r .DATABASE_URL)" \
        --from-literal=JWT_SECRET="$(aws secretsmanager get-secret-value --secret-id nexus-lms-prod/app-secrets --query SecretString --output text | jq -r .JWT_SECRET)" \
        --from-literal=JWT_REFRESH_SECRET="$(aws secretsmanager get-secret-value --secret-id nexus-lms-prod/app-secrets --query SecretString --output text | jq -r .JWT_REFRESH_SECRET)" \
        --dry-run=client -o yaml | kubectl apply -f -
    
    # Deploy with canary strategy
    - |
      cd k8s/overlays/production
      kustomize edit set image \
        nexus-client=$ECR_REGISTRY/$CLIENT_IMAGE:$CI_COMMIT_SHA \
        nexus-server=$ECR_REGISTRY/$SERVER_IMAGE:$CI_COMMIT_SHA
      kustomize build . | kubectl apply -f -
    
    - kubectl rollout status deployment/nexus-client-prod -n $KUBE_NAMESPACE_PROD --timeout=600s
    - kubectl rollout status deployment/nexus-server-prod -n $KUBE_NAMESPACE_PROD --timeout=600s
    
    # Database migrations
    - |
      kubectl run prisma-migrate-$CI_PIPELINE_ID \
        --namespace $KUBE_NAMESPACE_PROD \
        --image=$ECR_REGISTRY/$SERVER_IMAGE:$CI_COMMIT_SHA \
        --restart=Never \
        --rm \
        --wait \
        --command -- npx prisma migrate deploy
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
  needs:
    - deploy-staging

# ─────────────────────────────────────────
# Rollback Job
# ─────────────────────────────────────────
rollback-prod:
  stage: deploy-prod
  image: alpine/k8s:1.29.0
  script:
    - aws eks update-kubeconfig --region $AWS_DEFAULT_REGION --name $EKS_CLUSTER_PROD
    - kubectl rollout undo deployment/nexus-client-prod -n $KUBE_NAMESPACE_PROD
    - kubectl rollout undo deployment/nexus-server-prod -n $KUBE_NAMESPACE_PROD
    - kubectl rollout status deployment/nexus-client-prod -n $KUBE_NAMESPACE_PROD --timeout=300s
    - kubectl rollout status deployment/nexus-server-prod -n $KUBE_NAMESPACE_PROD --timeout=300s
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
  needs:
    - deploy-prod
