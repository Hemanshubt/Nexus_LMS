# ================================
# Nexus LMS - GitLab CI/CD Pipeline
# Complete DevOps Pipeline with Docker, K8s, Terraform, AWS
# ================================

stages:
  - validate
  - test
  - security
  - build
  - deploy-infrastructure
  - deploy-staging
  - deploy-production

# ================================
# Global Variables
# ================================
variables:
  # Docker configuration
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  
  # AWS Configuration
  AWS_REGION: ap-south-1
  AWS_ACCOUNT_ID: ${AWS_ACCOUNT_ID}
  
  # ECR Repositories
  ECR_SERVER_REPO: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/nexus-lms-server
  ECR_CLIENT_REPO: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/nexus-lms-client
  
  # EKS Cluster
  EKS_CLUSTER_NAME: nexus-lms-eks-prod
  
  # Kubernetes namespace
  K8S_NAMESPACE: nexus-lms
  
  # Terraform version
  TF_VERSION: "1.6.6"

# ================================
# Default Configuration
# ================================
default:
  image: alpine:latest
  tags:
    - docker
  retry:
    max: 2
    when:
      - runner_system_failure
      - stuck_or_timeout_failure

# ================================
# Cache Configuration
# ================================
.node_cache: &node_cache
  cache:
    key: ${CI_COMMIT_REF_SLUG}
    paths:
      - client/node_modules/
      - server/node_modules/
    policy: pull-push

# ================================
# Templates
# ================================
.aws_auth: &aws_auth
  before_script:
    - apk add --no-cache aws-cli curl jq
    - aws --version
    # Use OIDC authentication with AWS
    - >
      export AWS_WEB_IDENTITY_TOKEN_FILE=/tmp/web_identity_token
    - echo "${CI_JOB_JWT_V2}" > $AWS_WEB_IDENTITY_TOKEN_FILE
    - export AWS_ROLE_ARN="${AWS_ROLE_ARN}"
    - aws sts get-caller-identity

.docker_build: &docker_build
  image: docker:24-dind
  services:
    - docker:24-dind
  <<: *aws_auth

.kubectl_setup: &kubectl_setup
  before_script:
    - apk add --no-cache aws-cli curl
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    - aws eks update-kubeconfig --name ${EKS_CLUSTER_NAME} --region ${AWS_REGION}

# ================================
# STAGE: Validate
# ================================
validate:lint:client:
  stage: validate
  image: node:20-alpine
  <<: *node_cache
  script:
    - cd client
    - npm ci --prefer-offline
    - npm run lint
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

validate:typecheck:client:
  stage: validate
  image: node:20-alpine
  <<: *node_cache
  script:
    - cd client
    - npm ci --prefer-offline
    - npx tsc --noEmit
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

validate:typecheck:server:
  stage: validate
  image: node:20-alpine
  <<: *node_cache
  script:
    - cd server
    - npm ci --prefer-offline
    - npx tsc --noEmit
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

validate:terraform:
  stage: validate
  image: hashicorp/terraform:${TF_VERSION}
  script:
    - cd terraform
    - terraform init -backend=false
    - terraform validate
    - terraform fmt -check -recursive
  rules:
    - changes:
        - terraform/**/*
    - if: $CI_COMMIT_BRANCH == "main"

# ================================
# STAGE: Test
# ================================
test:server:
  stage: test
  image: node:20-alpine
  <<: *node_cache
  services:
    - postgres:16-alpine
    - redis:7-alpine
  variables:
    POSTGRES_DB: test_db
    POSTGRES_USER: test_user
    POSTGRES_PASSWORD: test_password
    DATABASE_URL: postgresql://test_user:test_password@postgres:5432/test_db
    REDIS_URL: redis://redis:6379
    JWT_SECRET: test-jwt-secret
  script:
    - cd server
    - npm ci --prefer-offline
    - npx prisma generate
    - npx prisma db push
    - npm test -- --coverage
  coverage: '/All files[^|]*\|[^|]*\s+([\d\.]+)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: server/coverage/cobertura-coverage.xml
    paths:
      - server/coverage/
    expire_in: 1 week
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

test:client:
  stage: test
  image: node:20-alpine
  <<: *node_cache
  script:
    - cd client
    - npm ci --prefer-offline
    - npm test -- --coverage --watchAll=false
  coverage: '/All files[^|]*\|[^|]*\s+([\d\.]+)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: client/coverage/cobertura-coverage.xml
    paths:
      - client/coverage/
    expire_in: 1 week
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# ================================
# STAGE: Security
# ================================
security:dependency-scan:
  stage: security
  image: node:20-alpine
  script:
    - cd client && npm audit --audit-level=high || true
    - cd ../server && npm audit --audit-level=high || true
  allow_failure: true
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

security:container-scan:server:
  stage: security
  image: aquasec/trivy:latest
  <<: *aws_auth
  script:
    - aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com
    - trivy image --exit-code 1 --severity HIGH,CRITICAL ${ECR_SERVER_REPO}:${CI_COMMIT_SHA}
  allow_failure: true
  needs:
    - build:server
  rules:
    - if: $CI_COMMIT_BRANCH == "main"

security:sast:
  stage: security
  image: semgrep/semgrep:latest
  script:
    - semgrep scan --config auto --sarif --output semgrep.sarif .
  artifacts:
    reports:
      sast: semgrep.sarif
    paths:
      - semgrep.sarif
    expire_in: 1 week
  allow_failure: true
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# ================================
# STAGE: Build
# ================================
build:server:
  stage: build
  <<: *docker_build
  script:
    # Login to ECR
    - aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com
    
    # Build and push
    - cd server
    - docker build 
        --cache-from ${ECR_SERVER_REPO}:latest 
        --build-arg BUILDKIT_INLINE_CACHE=1
        -t ${ECR_SERVER_REPO}:${CI_COMMIT_SHA}
        -t ${ECR_SERVER_REPO}:${CI_COMMIT_REF_SLUG}
        -t ${ECR_SERVER_REPO}:latest
        .
    - docker push ${ECR_SERVER_REPO}:${CI_COMMIT_SHA}
    - docker push ${ECR_SERVER_REPO}:${CI_COMMIT_REF_SLUG}
    - docker push ${ECR_SERVER_REPO}:latest
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
    - if: $CI_COMMIT_TAG

build:client:
  stage: build
  <<: *docker_build
  script:
    # Login to ECR
    - aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com
    
    # Build and push
    - cd client
    - docker build 
        --cache-from ${ECR_CLIENT_REPO}:latest 
        --build-arg BUILDKIT_INLINE_CACHE=1
        --build-arg VITE_API_URL=${VITE_API_URL}
        -t ${ECR_CLIENT_REPO}:${CI_COMMIT_SHA}
        -t ${ECR_CLIENT_REPO}:${CI_COMMIT_REF_SLUG}
        -t ${ECR_CLIENT_REPO}:latest
        .
    - docker push ${ECR_CLIENT_REPO}:${CI_COMMIT_SHA}
    - docker push ${ECR_CLIENT_REPO}:${CI_COMMIT_REF_SLUG}
    - docker push ${ECR_CLIENT_REPO}:latest
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
    - if: $CI_COMMIT_TAG

# ================================
# STAGE: Deploy Infrastructure
# ================================
terraform:plan:
  stage: deploy-infrastructure
  image: hashicorp/terraform:${TF_VERSION}
  <<: *aws_auth
  script:
    - cd terraform
    - terraform init
    - terraform plan -out=tfplan
  artifacts:
    paths:
      - terraform/tfplan
    expire_in: 1 day
  rules:
    - changes:
        - terraform/**/*
      when: manual
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual

terraform:apply:
  stage: deploy-infrastructure
  image: hashicorp/terraform:${TF_VERSION}
  <<: *aws_auth
  script:
    - cd terraform
    - terraform init
    - terraform apply -auto-approve tfplan
  needs:
    - terraform:plan
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
  environment:
    name: infrastructure

# ================================
# STAGE: Deploy Staging
# ================================
deploy:staging:
  stage: deploy-staging
  image: alpine:latest
  <<: *kubectl_setup
  variables:
    ENVIRONMENT: staging
    K8S_NAMESPACE: nexus-lms-staging
  script:
    # Update image tags in manifests
    - export IMAGE_TAG=${CI_COMMIT_SHA}
    
    # Apply Kubernetes manifests
    - kubectl apply -f k8s/namespace.yaml
    - kubectl apply -f k8s/configmap.yaml
    - kubectl apply -f k8s/secrets.yaml -n ${K8S_NAMESPACE}
    - kubectl apply -f k8s/rbac.yaml
    
    # Deploy with image substitution
    - |
      sed -e "s|\${AWS_ACCOUNT_ID}|${AWS_ACCOUNT_ID}|g" \
          -e "s|\${AWS_REGION}|${AWS_REGION}|g" \
          -e "s|:latest|:${IMAGE_TAG}|g" \
          k8s/server-deployment.yaml | kubectl apply -f -
    
    - |
      sed -e "s|\${AWS_ACCOUNT_ID}|${AWS_ACCOUNT_ID}|g" \
          -e "s|\${AWS_REGION}|${AWS_REGION}|g" \
          -e "s|:latest|:${IMAGE_TAG}|g" \
          k8s/client-deployment.yaml | kubectl apply -f -
    
    - kubectl apply -f k8s/ingress.yaml
    - kubectl apply -f k8s/network-policies.yaml
    - kubectl apply -f k8s/pdb.yaml
    
    # Wait for rollout
    - kubectl rollout status deployment/nexus-lms-server -n ${K8S_NAMESPACE} --timeout=300s
    - kubectl rollout status deployment/nexus-lms-client -n ${K8S_NAMESPACE} --timeout=300s
  environment:
    name: staging
    url: https://staging.nexus-lms.example.com
  needs:
    - build:server
    - build:client
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"
      when: on_success

# ================================
# STAGE: Deploy Production
# ================================
deploy:production:
  stage: deploy-production
  image: alpine:latest
  <<: *kubectl_setup
  variables:
    ENVIRONMENT: production
    K8S_NAMESPACE: nexus-lms
  script:
    # Update image tags in manifests
    - export IMAGE_TAG=${CI_COMMIT_SHA}
    
    # Apply Kubernetes manifests
    - kubectl apply -f k8s/namespace.yaml
    - kubectl apply -f k8s/configmap.yaml
    - kubectl apply -f k8s/secrets.yaml -n ${K8S_NAMESPACE}
    - kubectl apply -f k8s/rbac.yaml
    
    # Deploy with image substitution
    - |
      sed -e "s|\${AWS_ACCOUNT_ID}|${AWS_ACCOUNT_ID}|g" \
          -e "s|\${AWS_REGION}|${AWS_REGION}|g" \
          -e "s|:latest|:${IMAGE_TAG}|g" \
          k8s/server-deployment.yaml | kubectl apply -f -
    
    - |
      sed -e "s|\${AWS_ACCOUNT_ID}|${AWS_ACCOUNT_ID}|g" \
          -e "s|\${AWS_REGION}|${AWS_REGION}|g" \
          -e "s|:latest|:${IMAGE_TAG}|g" \
          k8s/client-deployment.yaml | kubectl apply -f -
    
    - kubectl apply -f k8s/ingress.yaml
    - kubectl apply -f k8s/network-policies.yaml
    - kubectl apply -f k8s/pdb.yaml
    
    # Wait for rollout
    - kubectl rollout status deployment/nexus-lms-server -n ${K8S_NAMESPACE} --timeout=300s
    - kubectl rollout status deployment/nexus-lms-client -n ${K8S_NAMESPACE} --timeout=300s
    
    # Verify deployment
    - kubectl get pods -n ${K8S_NAMESPACE}
  environment:
    name: production
    url: https://nexus-lms.example.com
  needs:
    - build:server
    - build:client
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
    - if: $CI_COMMIT_TAG
      when: manual

# ================================
# Rollback Job
# ================================
rollback:production:
  stage: deploy-production
  image: alpine:latest
  <<: *kubectl_setup
  script:
    - kubectl rollout undo deployment/nexus-lms-server -n ${K8S_NAMESPACE}
    - kubectl rollout undo deployment/nexus-lms-client -n ${K8S_NAMESPACE}
    - kubectl rollout status deployment/nexus-lms-server -n ${K8S_NAMESPACE} --timeout=300s
    - kubectl rollout status deployment/nexus-lms-client -n ${K8S_NAMESPACE} --timeout=300s
  environment:
    name: production
    action: stop
  when: manual
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
